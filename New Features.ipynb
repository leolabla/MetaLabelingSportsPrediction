{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7632f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Step 1: Download dataset (downloads to ~/.kagglehub)\n",
    "dataset_path = kagglehub.dataset_download('hugomathien/soccer')\n",
    "\n",
    "# Step 2: Define your flat target folder (e.g., 'data')\n",
    "target_path = 'data'\n",
    "\n",
    "# Step 3: Copy all files (flat) into the target folder\n",
    "if not os.path.exists(target_path):\n",
    "    os.makedirs(target_path)\n",
    "\n",
    "for filename in os.listdir(dataset_path):\n",
    "    src_file = os.path.join(dataset_path, filename)\n",
    "    dst_file = os.path.join(target_path, filename)\n",
    "\n",
    "    if os.path.isfile(src_file):  # ignore folders\n",
    "        shutil.copy2(src_file, dst_file)\n",
    "\n",
    "print(f\"All dataset files copied to: {target_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88aba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import unicodedata\n",
    "import requests\n",
    "from datetime import date, datetime\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading all functions\n",
    "def get_match_label(match):\n",
    "    ''' Derives a label for a given match. '''\n",
    "\n",
    "    #Define variables\n",
    "    home_goals = match['home_team_goal']\n",
    "    away_goals = match['away_team_goal']\n",
    "\n",
    "    label = pd.DataFrame()\n",
    "    label.loc[0,'match_api_id'] = match['match_api_id']\n",
    "\n",
    "    #Identify match label\n",
    "    if home_goals > away_goals:\n",
    "        label.loc[0,'label'] = \"Win\"\n",
    "    if home_goals == away_goals:\n",
    "        label.loc[0,'label'] = \"Draw\"\n",
    "    if home_goals < away_goals:\n",
    "        label.loc[0,'label'] = \"Defeat\"\n",
    "\n",
    "    #Return label\n",
    "    return label.loc[0]\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_fifa_stats(match, player_stats_lookup):\n",
    "    ''' Efficiently aggregates FIFA stats for a given match. '''\n",
    "\n",
    "    match_id = match.match_api_id\n",
    "    date = match['date']\n",
    "    players = ['home_player_1', 'home_player_2', 'home_player_3', \"home_player_4\", \"home_player_5\",\n",
    "               \"home_player_6\", \"home_player_7\", \"home_player_8\", \"home_player_9\", \"home_player_10\",\n",
    "               \"home_player_11\", \"away_player_1\", \"away_player_2\", \"away_player_3\", \"away_player_4\",\n",
    "               \"away_player_5\", \"away_player_6\", \"away_player_7\", \"away_player_8\", \"away_player_9\",\n",
    "               \"away_player_10\", \"away_player_11\"]\n",
    "\n",
    "    row = {}\n",
    "    for player in players:\n",
    "        player_id = match[player]\n",
    "\n",
    "        if np.isnan(player_id):\n",
    "            row[f\"{player}_overall_rating\"] = 0\n",
    "            continue\n",
    "\n",
    "        # Use pre-sorted lookup table (dict of DataFrames)\n",
    "        stats = player_stats_lookup.get(player_id)\n",
    "        if stats is not None:\n",
    "            # Use .searchsorted for fast filtering by date\n",
    "            idx = stats['date'].searchsorted(date, side='left') - 1\n",
    "            if idx >= 0:\n",
    "                row[f\"{player}_overall_rating\"] = stats.iloc[idx]['overall_rating']\n",
    "            else:\n",
    "                row[f\"{player}_overall_rating\"] = 0\n",
    "        else:\n",
    "            row[f\"{player}_overall_rating\"] = 0\n",
    "\n",
    "    row['match_api_id'] = match_id\n",
    "    return pd.Series(row)\n",
    "\n",
    "def build_player_stats_lookup(player_stats):\n",
    "    ''' Build a pre-sorted dictionary of player stats for quick access. '''\n",
    "    player_stats = player_stats.sort_values(['player_api_id', 'date'])\n",
    "    return {\n",
    "        pid: df.reset_index(drop=True)\n",
    "        for pid, df in player_stats.groupby('player_api_id')\n",
    "    }\n",
    "\n",
    "\n",
    "def get_fifa_data(matches, player_stats, path=None, data_exists=False):\n",
    "    ''' Gets fifa data for all matches. '''\n",
    "\n",
    "    if data_exists and path:\n",
    "        return pd.read_pickle(path)\n",
    "\n",
    "    print(\"Collecting FIFA data for each match...\")\n",
    "    player_stats_lookup = build_player_stats_lookup(player_stats)\n",
    "\n",
    "    fifa_data = matches.apply(\n",
    "        lambda row: get_fifa_stats(row, player_stats_lookup), axis=1\n",
    "    )\n",
    "    \n",
    "    return fifa_data\n",
    "\n",
    "def get_overall_fifa_rankings(fifa, get_overall = False):\n",
    "    ''' Get overall fifa rankings from fifa data. '''\n",
    "\n",
    "    temp_data = fifa\n",
    "\n",
    "    #Check if only overall player stats are desired\n",
    "    if get_overall == True:\n",
    "\n",
    "        #Get overall stats\n",
    "        data = temp_data.loc[:,(fifa.columns.str.contains('overall_rating'))]\n",
    "        data.loc[:,'match_api_id'] = temp_data.loc[:,'match_api_id']\n",
    "    else:\n",
    "\n",
    "        #Get all stats except for stat date\n",
    "        cols = fifa.loc[:,(fifa.columns.str.contains('date_stat'))]\n",
    "        temp_data = fifa.drop(cols.columns, axis = 1)\n",
    "        data = temp_data\n",
    "\n",
    "    #Return data\n",
    "    return data\n",
    "\n",
    "def get_last_matches(matches, date, team, x = 10):\n",
    "    ''' Get the last x matches of a given team. '''\n",
    "\n",
    "    #Filter team matches from matches\n",
    "    team_matches = matches[(matches['home_team_api_id'] == team) | (matches['away_team_api_id'] == team)]\n",
    "\n",
    "    #Filter x last matches from team matches\n",
    "    last_matches = team_matches[team_matches.date < date].sort_values(by = 'date', ascending = False).iloc[0:x,:]\n",
    "\n",
    "    #Return last matches\n",
    "    return last_matches\n",
    "\n",
    "def get_last_matches_against_eachother(matches, date, home_team, away_team, x = 10):\n",
    "    ''' Get the last x matches of two given teams. '''\n",
    "\n",
    "    #Find matches of both teams\n",
    "    home_matches = matches[(matches['home_team_api_id'] == home_team) & (matches['away_team_api_id'] == away_team)]\n",
    "    away_matches = matches[(matches['home_team_api_id'] == away_team) & (matches['away_team_api_id'] == home_team)]\n",
    "    total_matches = pd.concat([home_matches, away_matches])\n",
    "\n",
    "    #Get last x matches\n",
    "    try:\n",
    "        last_matches = total_matches[total_matches.date < date].sort_values(by = 'date', ascending = False).iloc[0:x,:]\n",
    "    except:\n",
    "        last_matches = total_matches[total_matches.date < date].sort_values(by = 'date', ascending = False).iloc[0:total_matches.shape[0],:]\n",
    "\n",
    "        #Check for error in data\n",
    "        if(last_matches.shape[0] > x):\n",
    "            print(\"Error in obtaining matches\")\n",
    "\n",
    "    #Return data\n",
    "    return last_matches\n",
    "\n",
    "def get_goals(matches, team):\n",
    "    ''' Get the goals of a specfic team from a set of matches. '''\n",
    "\n",
    "    #Find home and away goals\n",
    "    home_goals = int(matches.home_team_goal[matches.home_team_api_id == team].sum())\n",
    "    away_goals = int(matches.away_team_goal[matches.away_team_api_id == team].sum())\n",
    "\n",
    "    total_goals = home_goals + away_goals\n",
    "\n",
    "    #Return total goals\n",
    "    return total_goals\n",
    "\n",
    "def get_goals_conceided(matches, team):\n",
    "    ''' Get the goals conceided of a specfic team from a set of matches. '''\n",
    "\n",
    "    #Find home and away goals\n",
    "    home_goals = int(matches.home_team_goal[matches.away_team_api_id == team].sum())\n",
    "    away_goals = int(matches.away_team_goal[matches.home_team_api_id == team].sum())\n",
    "\n",
    "    total_goals = home_goals + away_goals\n",
    "\n",
    "    #Return total goals\n",
    "    return total_goals\n",
    "\n",
    "def get_wins(matches, team):\n",
    "    ''' Get the number of wins of a specfic team from a set of matches. '''\n",
    "\n",
    "    #Find home and away wins\n",
    "    home_wins = int(matches.home_team_goal[(matches.home_team_api_id == team) & (matches.home_team_goal > matches.away_team_goal)].count())\n",
    "    away_wins = int(matches.away_team_goal[(matches.away_team_api_id == team) & (matches.away_team_goal > matches.home_team_goal)].count())\n",
    "\n",
    "    total_wins = home_wins + away_wins\n",
    "\n",
    "    #Return total wins\n",
    "    return total_wins\n",
    "\n",
    "def get_match_features(match, team_histories):\n",
    "    date = match.date\n",
    "    home_team = match.home_team_api_id\n",
    "    away_team = match.away_team_api_id\n",
    "\n",
    "    home_matches = get_recent_team_matches(team_histories, home_team, date, 10)\n",
    "    away_matches = get_recent_team_matches(team_histories, away_team, date, 10)\n",
    "\n",
    "    goals_scored_home = get_goals(home_matches, home_team)\n",
    "    goals_scored_away = get_goals(away_matches, away_team)\n",
    "    goals_conceded_home = get_goals_conceided(home_matches, home_team)\n",
    "    goals_conceded_away = get_goals_conceided(away_matches, away_team)\n",
    "\n",
    "    return {\n",
    "        'match_api_id': match.match_api_id,\n",
    "        'league_id': match.league_id,\n",
    "        'home_team_goals_difference': goals_scored_home - goals_conceded_home,\n",
    "        'away_team_goals_difference': goals_scored_away - goals_conceded_away,\n",
    "        'games_won_home_team': get_wins(home_matches, home_team),\n",
    "        'games_won_away_team': get_wins(away_matches, away_team),\n",
    "        # Optional: precompute `against` matches similarly\n",
    "        'games_against_won': 0,\n",
    "        'games_against_lost': 0,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_feables(matches, fifa, bookkeepers, get_overall = False, horizontal = True, x = 10, verbose = True):\n",
    "    ''' Create and aggregate features and labels for all matches. '''\n",
    "\n",
    "    #Get fifa stats features\n",
    "    fifa_stats = get_overall_fifa_rankings(fifa, get_overall)\n",
    "\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"Generating match features...\")\n",
    "    start = time()\n",
    "\n",
    "    #Get match features for all matches\n",
    "    team_histories = precompute_last_matches(matches)\n",
    "    print(\"Preindex_match_data\")\n",
    "    match_stats = matches.apply(lambda row: pd.Series(get_match_features(row, team_histories)), axis=1)    \n",
    "    print(\"apply {:.1f} minutes\".format((time() - start)/60))\n",
    "    #Create dummies for league ID feature\n",
    "    dummies = pd.get_dummies(match_stats['league_id']).rename(columns = lambda x: 'League_' + str(x))\n",
    "    print(\"dummy {:.1f} minutes\".format((time() - start)/60))\n",
    "    match_stats = pd.concat([match_stats, dummies], axis = 1)\n",
    "    print(\"concat {:.1f} minutes\".format((time() - start)/60))\n",
    "    match_stats.drop(['league_id'], inplace = True, axis = 1)\n",
    "\n",
    "    end = time()\n",
    "    if verbose == True:\n",
    "        print(\"Match features generated in {:.1f} minutes\".format((end - start)/60))\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"Generating match labels...\")\n",
    "    start = time()\n",
    "\n",
    "    #Create match labels\n",
    "    labels = matches.apply(get_match_label, axis = 1)\n",
    "    end = time()\n",
    "    if verbose == True:\n",
    "        print(\"Match labels generated in {:.1f} minutes\".format((end - start)/60))\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"Generating bookkeeper data...\")\n",
    "    start = time()\n",
    "\n",
    "    #Get bookkeeper quotas for all matches\n",
    "    bk_data = get_bookkeeper_data(matches, bookkeepers, horizontal = True)\n",
    "    bk_data.loc[:,'match_api_id'] = matches.loc[:,'match_api_id']\n",
    "    end = time()\n",
    "    if verbose == True:\n",
    "        print(\"Bookkeeper data generated in {:.1f} minutes\".format((end - start)/60))\n",
    "\n",
    "    #Merges features and labels into one frame\n",
    "    features = pd.merge(match_stats, fifa_stats, on = 'match_api_id', how = 'left')\n",
    "    features = pd.merge(features, bk_data, on = 'match_api_id', how = 'left')\n",
    "    feables = pd.merge(features, labels, on = 'match_api_id', how = 'left')\n",
    "\n",
    "    #Drop NA values\n",
    "    feables.dropna(inplace = True)\n",
    "\n",
    "    #Return preprocessed data\n",
    "    return feables\n",
    "\n",
    "def precompute_last_matches(matches, x=10):\n",
    "    matches_sorted = matches.sort_values(by='date')\n",
    "    team_histories = {}\n",
    "\n",
    "    for team in pd.concat([matches_sorted['home_team_api_id'], matches_sorted['away_team_api_id']]).unique():\n",
    "        team_matches = matches_sorted[(matches['home_team_api_id'] == team) | (matches_sorted['away_team_api_id'] == team)]\n",
    "        team_histories[team] = team_matches.sort_values('date').reset_index(drop=True)\n",
    "    return team_histories\n",
    "\n",
    "def get_recent_team_matches(team_histories, team_id, current_date, x):\n",
    "    team_matches = team_histories.get(team_id, pd.DataFrame())\n",
    "    return team_matches[team_matches['date'] < current_date].tail(x)\n",
    "\n",
    "def train_classifier(clf, dm_reduction, X_train, y_train, cv_sets, params, scorer, jobs, use_grid_search = True,\n",
    "                     best_components = None, best_params = None):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "\n",
    "    #Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "\n",
    "    #Check if grid search should be applied\n",
    "    if use_grid_search == True:\n",
    "\n",
    "        #Define pipeline of dm reduction and classifier\n",
    "        estimators = [('dm_reduce', dm_reduction), ('clf', clf)]\n",
    "        pipeline = Pipeline(estimators)\n",
    "\n",
    "        #Grid search over pipeline and return best classifier\n",
    "        grid_obj = model_selection.GridSearchCV(pipeline, param_grid = params, scoring = scorer, cv = cv_sets, n_jobs = jobs)\n",
    "        grid_obj.fit(X_train, y_train)\n",
    "        best_pipe = grid_obj.best_estimator_\n",
    "    else:\n",
    "\n",
    "        #Use best components that are known without grid search\n",
    "        estimators = [('dm_reduce', dm_reduction(n_components = best_components)), ('clf', clf(best_params))]\n",
    "        pipeline = Pipeline(estimators)\n",
    "        best_pipe = pipeline.fit(X_train, y_train)\n",
    "\n",
    "    end = time()\n",
    "\n",
    "    #Print the results\n",
    "    print(\"Trained {} in {:.1f} minutes\".format(clf.__class__.__name__, (end - start)/60))\n",
    "\n",
    "    #Return best pipe\n",
    "    return best_pipe\n",
    "\n",
    "def predict_labels(clf, best_pipe, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on scorer. '''\n",
    "\n",
    "    #Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(best_pipe.named_steps['dm_reduce'].transform(features))\n",
    "    end = time()\n",
    "\n",
    "    #Print and return results\n",
    "    print(\"Made predictions in {:.4f} seconds\".format(end - start))\n",
    "    return accuracy_score(target.values, y_pred)\n",
    "\n",
    "def train_calibrate_predict(clf, dm_reduction, X_train, y_train, X_calibrate, y_calibrate, X_test, y_test, cv_sets, params, scorer, jobs,\n",
    "                            use_grid_search = True, **kwargs):\n",
    "    ''' Train and predict using a classifer based on scorer. '''\n",
    "\n",
    "    #Indicate the classifier and the training set size\n",
    "    print(\"Training a {} with {}...\".format(clf.__class__.__name__, dm_reduction.__class__.__name__))\n",
    "\n",
    "    #Train the classifier\n",
    "    best_pipe = train_classifier(clf, dm_reduction, X_train, y_train, cv_sets, params, scorer, jobs)\n",
    "\n",
    "    #Calibrate classifier\n",
    "    print(\"Calibrating probabilities of classifier...\")\n",
    "    start = time()\n",
    "    clf = CalibratedClassifierCV(best_pipe.named_steps['clf'], cv= 'prefit', method='isotonic')\n",
    "    clf.fit(best_pipe.named_steps['dm_reduce'].transform(X_calibrate), y_calibrate)\n",
    "    end = time()\n",
    "    print(\"Calibrated {} in {:.1f} minutes\".format(clf.__class__.__name__, (end - start)/60))\n",
    "\n",
    "    # Print the results of prediction for both training and testing\n",
    "    print(\"Score of {} for training set: {:.4f}.\".format(clf.__class__.__name__, predict_labels(clf, best_pipe, X_train, y_train)))\n",
    "    print(\"Score of {} for test set: {:.4f}.\".format(clf.__class__.__name__, predict_labels(clf, best_pipe, X_test, y_test)))\n",
    "\n",
    "    #Return classifier, dm reduction, and label predictions for train and test set\n",
    "    return clf, best_pipe.named_steps['dm_reduce'], predict_labels(clf, best_pipe, X_train, y_train), predict_labels(clf, best_pipe, X_test, y_test)\n",
    "\n",
    "def convert_odds_to_prob(match_odds):\n",
    "    ''' Converts bookkeeper odds to probabilities. '''\n",
    "\n",
    "    #Define variables\n",
    "    match_id = match_odds.loc[:,'match_api_id']\n",
    "    bookkeeper = match_odds.loc[:,'bookkeeper']\n",
    "    win_odd = match_odds.loc[:,'Win']\n",
    "    draw_odd = match_odds.loc[:,'Draw']\n",
    "    loss_odd = match_odds.loc[:,'Defeat']\n",
    "\n",
    "    #Converts odds to prob\n",
    "    win_prob = 1 / win_odd\n",
    "    draw_prob = 1 / draw_odd\n",
    "    loss_prob = 1 / loss_odd\n",
    "\n",
    "    total_prob = win_prob + draw_prob + loss_prob\n",
    "\n",
    "    probs = pd.DataFrame()\n",
    "\n",
    "    #Define output format and scale probs by sum over all probs\n",
    "    probs.loc[:,'match_api_id'] = match_id\n",
    "    probs.loc[:,'bookkeeper'] = bookkeeper\n",
    "    probs.loc[:,'Win'] = win_prob / total_prob\n",
    "    probs.loc[:,'Draw'] = draw_prob / total_prob\n",
    "    probs.loc[:,'Defeat'] = loss_prob / total_prob\n",
    "\n",
    "    #Return probs and meta data\n",
    "    return probs\n",
    "\n",
    "def get_bookkeeper_data(matches, bookkeepers, horizontal = True):\n",
    "    ''' Aggregates bookkeeper data for all matches and bookkeepers. '''\n",
    "\n",
    "    bk_data = pd.DataFrame()\n",
    "\n",
    "    #Loop through bookkeepers\n",
    "    for bookkeeper in bookkeepers:\n",
    "\n",
    "        #Find columns containing data of bookkeeper\n",
    "        temp_data = matches.loc[:,(matches.columns.str.contains(bookkeeper))]\n",
    "        temp_data.loc[:, 'bookkeeper'] = str(bookkeeper)\n",
    "        temp_data.loc[:, 'match_api_id'] = matches.loc[:, 'match_api_id']\n",
    "\n",
    "        #Rename odds columns and convert to numeric\n",
    "        cols = temp_data.columns.values\n",
    "        cols[:3] = ['Win','Draw','Defeat']\n",
    "        temp_data.columns = cols\n",
    "        temp_data.loc[:,'Win'] = pd.to_numeric(temp_data['Win'])\n",
    "        temp_data.loc[:,'Draw'] = pd.to_numeric(temp_data['Draw'])\n",
    "        temp_data.loc[:,'Defeat'] = pd.to_numeric(temp_data['Defeat'])\n",
    "\n",
    "        #Check if data should be aggregated horizontally\n",
    "        if(horizontal == True):\n",
    "\n",
    "            #Convert data to probs\n",
    "            #temp_data = convert_odds_to_prob(temp_data)\n",
    "            temp_data.drop('match_api_id', axis = 1, inplace = True)\n",
    "            temp_data.drop('bookkeeper', axis = 1, inplace = True)\n",
    "\n",
    "            #Rename columns with bookkeeper names\n",
    "            win_name = bookkeeper + \"_\" + \"Win\"\n",
    "            draw_name = bookkeeper + \"_\" + \"Draw\"\n",
    "            defeat_name = bookkeeper + \"_\" + \"Defeat\"\n",
    "            temp_data.columns.values[:3] = [win_name, draw_name, defeat_name]\n",
    "\n",
    "            #Aggregate data\n",
    "            bk_data = pd.concat([bk_data, temp_data], axis = 1)\n",
    "        else:\n",
    "            #Aggregate vertically\n",
    "            bk_data = pd.concat([bk_data, temp_data], ignore_index=True)\n",
    "\n",
    "    #If horizontal add match api id to data\n",
    "    if(horizontal == True):\n",
    "        temp_data.loc[:, 'match_api_id'] = matches.loc[:, 'match_api_id']\n",
    "\n",
    "    #Return bookkeeper data\n",
    "    return bk_data\n",
    "\n",
    "def get_bookkeeper_probs(matches, bookkeepers, horizontal = False):\n",
    "    ''' Get bookkeeper data and convert to probabilities for vertical aggregation. '''\n",
    "\n",
    "    #Get bookkeeper data\n",
    "    data = get_bookkeeper_data(matches, bookkeepers, horizontal = False)\n",
    "\n",
    "    #Convert odds to probabilities\n",
    "    probs = convert_odds_to_prob(data)\n",
    "\n",
    "    #Return data\n",
    "    return probs\n",
    "\n",
    "def plot_confusion_matrix(y_test, X_test, clf, dim_reduce, path, cmap=plt.cm.Blues, normalize = False):\n",
    "    labels = [\"Win\", \"Draw\", \"Defeat\"]\n",
    "    cm = confusion_matrix(y_test, clf.predict(dim_reduce.transform(X_test)), labels=labels)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum()\n",
    "\n",
    "    #Configure figure\n",
    "    sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "    fig = plt.figure(1)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap = plt.cm.Blues)\n",
    "    # Corrected attribute name: Use .estimator instead of .base_estimator\n",
    "    title = \"Confusion matrix of a {} with {}\".format(\n",
    "    getattr(clf, 'estimator', clf).__class__.__name__,\n",
    "    dim_reduce.__class__.__name__)\n",
    "\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #Print classification report\n",
    "    y_pred = clf.predict(dim_reduce.transform(X_test))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "def compare_probabilities(clf, dim_reduce, bk, bookkeepers, matches, fifa_data, verbose = False):\n",
    "    ''' Map bookkeeper and model probabilities. '''\n",
    "\n",
    "    #Create features and labels for given matches\n",
    "    feables = create_feables(matches, fifa_data, bk, get_overall = True, verbose = False)\n",
    "\n",
    "    #Ensure consistency\n",
    "    match_ids = list(feables['match_api_id'])\n",
    "    matches = matches[matches['match_api_id'].isin(match_ids)]\n",
    "\n",
    "    #Get bookkeeper probabilities\n",
    "    if verbose == True:\n",
    "        print(\"Obtaining bookkeeper probabilities...\")\n",
    "    bookkeeper_probs = get_bookkeeper_probs(matches, bookkeepers)\n",
    "    bookkeeper_probs.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    inputs = feables.drop('match_api_id', axis = 1)\n",
    "    labels = inputs.loc[:,'label']\n",
    "    features = inputs.drop('label', axis = 1)\n",
    "\n",
    "    #Get model probabilities\n",
    "    if verbose == True:\n",
    "        print(\"Predicting probabilities based on model...\")\n",
    "    model_probs = pd.DataFrame()\n",
    "    label_table = pd.Series()\n",
    "    temp_probs = pd.DataFrame(clf.predict_proba(dim_reduce.transform(features)), columns = ['win_prob', 'draw_prob', 'defeat_prob'])\n",
    "    for bookkeeper in bookkeepers:\n",
    "        model_probs = pd.concat([model_probs, temp_probs], ignore_index=True)\n",
    "        label_table = pd.concat([label_table, labels], ignore_index=True)\n",
    "    model_probs.reset_index(inplace = True, drop = True)\n",
    "    label_table.reset_index(inplace = True, drop = True)\n",
    "    bookkeeper_probs['win_prob'] = model_probs['win_prob']\n",
    "    bookkeeper_probs['draw_prob'] = model_probs['draw_prob']\n",
    "    bookkeeper_probs['defeat_prob'] = model_probs['defeat_prob']\n",
    "    bookkeeper_probs['label'] = label_table\n",
    "\n",
    "    #Aggregate win probabilities for each match\n",
    "    wins = bookkeeper_probs[['bookkeeper', 'match_api_id', 'Win', 'win_prob', 'label']]\n",
    "    wins.loc[:, 'bet'] = 'Win'\n",
    "    wins = wins.rename(columns = {'Win':'bookkeeper_prob',\n",
    "                                  'win_prob': 'model_prob'})\n",
    "\n",
    "    #Aggregate draw probabilities for each match\n",
    "    draws = bookkeeper_probs[['bookkeeper', 'match_api_id', 'Draw', 'draw_prob', 'label']]\n",
    "    draws.loc[:, 'bet'] = 'Draw'\n",
    "    draws = draws.rename(columns = {'Draw':'bookkeeper_prob',\n",
    "                                  'draw_prob': 'model_prob'})\n",
    "\n",
    "    #Aggregate defeat probabilities for each match\n",
    "    defeats = bookkeeper_probs[['bookkeeper', 'match_api_id', 'Defeat', 'defeat_prob', 'label']]\n",
    "    defeats.loc[:, 'bet'] = 'Defeat'\n",
    "    defeats = defeats.rename(columns = {'Defeat':'bookkeeper_prob',\n",
    "                                  'defeat_prob': 'model_prob'})\n",
    "\n",
    "    total = pd.concat([wins, draws, defeats])\n",
    "\n",
    "    #Return total\n",
    "    return total\n",
    "\n",
    "def find_good_bets(clf, dim_reduce, bk, bookkeepers, matches, fifa_data, percentile, prob_cap, verbose = False):\n",
    "    ''' Find good bets for a given classifier and matches. '''\n",
    "\n",
    "    #Compare model and classifier probabilities\n",
    "    probs = compare_probabilities(clf, dim_reduce, bk, bookkeepers, matches, fifa_data, verbose = False)\n",
    "    probs.loc[:, 'prob_difference'] = probs.loc[:,\"model_prob\"] - probs.loc[:,\"bookkeeper_prob\"]\n",
    "\n",
    "    #Sort by createst difference to identify most underestimated bets\n",
    "    values = probs['prob_difference']\n",
    "    values = values.sort_values(ascending = False)\n",
    "    values.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"Selecting attractive bets...\")\n",
    "\n",
    "    #Identify choices that fulfill requirements such as positive difference, minimum probability and match outcome\n",
    "    relevant_choices = probs[(probs.prob_difference > 0) & (probs.model_prob > prob_cap) & (probs.bet != \"Draw\")]\n",
    "\n",
    "    #Select given percentile of relevant choices\n",
    "    top_percent = 1 - percentile\n",
    "    choices = relevant_choices[relevant_choices.prob_difference >= relevant_choices.prob_difference.quantile(top_percent)]\n",
    "    choices.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    #Return choices\n",
    "    return choices\n",
    "\n",
    "def get_reward(choice, matches):\n",
    "    ''' Get the reward of a given bet. '''\n",
    "\n",
    "    #Identify bet\n",
    "    match = matches[matches.match_api_id == choice.match_api_id]\n",
    "    bet_data = match.loc[:,(match.columns.str.contains(choice.bookkeeper))]\n",
    "    cols = bet_data.columns.values\n",
    "    cols[:3] = ['win','draw','defeat']\n",
    "    bet_data.columns = cols\n",
    "\n",
    "    #Identfiy bet type and get quota\n",
    "    if choice.bet == 'Win':\n",
    "        bet_quota = bet_data.win.values\n",
    "    elif choice.bet == 'Draw':\n",
    "        bet_quota = bet_data.draw.values\n",
    "    elif choice.bet == 'Defeat':\n",
    "        bet_quota = bet_data.defeat.values\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "\n",
    "    #Check label and compute reward\n",
    "    if choice.bet == choice.label:\n",
    "        reward = bet_quota\n",
    "    else:\n",
    "        reward = 0\n",
    "\n",
    "    #Return reward\n",
    "    return reward\n",
    "\n",
    "def execute_bets(bet_choices, matches, verbose = False):\n",
    "    ''' Get rewards for all bets. '''\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"Obtaining reward for chosen bets...\")\n",
    "    total_reward = 0\n",
    "    total_invested = 0\n",
    "\n",
    "    #Loop through bets\n",
    "    loops = np.arange(0, bet_choices.shape[0])\n",
    "    for i in loops:\n",
    "\n",
    "        #Get rewards and accumulate profit\n",
    "        reward = get_reward(bet_choices.iloc[i,:], matches)\n",
    "        total_reward = total_reward + reward\n",
    "        total_invested += 1\n",
    "\n",
    "    #Compute investment return\n",
    "    investment_return = float(total_reward / total_invested) - 1\n",
    "\n",
    "    #Return investment return\n",
    "    return investment_return\n",
    "\n",
    "def explore_data(features, inputs, path):\n",
    "    ''' Explore data by plotting KDE graphs. '''\n",
    "\n",
    "    #Define figure subplots\n",
    "    fig = plt.figure(1)\n",
    "    fig.subplots_adjust(bottom= -1, left=0.025, top = 2, right=0.975)\n",
    "\n",
    "    #Loop through features\n",
    "    i = 1\n",
    "    for col in features.columns:\n",
    "\n",
    "        #Set subplot and plot format\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.set_context(\"paper\", font_scale = 0.5, rc={\"lines.linewidth\": 1})\n",
    "        plt.subplot(7,7,0 + i)\n",
    "        j = i - 1\n",
    "\n",
    "        #Plot KDE for all labels\n",
    "        sns.kdeplot(data=inputs[inputs['label'] == 'Win'].iloc[:,j], label='Win', fill=True)\n",
    "        sns.kdeplot(data=inputs[inputs['label'] == 'Draw'].iloc[:,j], label='Draw', fill=True)\n",
    "        sns.kdeplot(data=inputs[inputs['label'] == 'Defeat'].iloc[:,j], label='Defeat', fill=True)\n",
    "        plt.legend();\n",
    "        i = i + 1\n",
    "\n",
    "    #Define plot format\n",
    "    DefaultSize = fig.get_size_inches()\n",
    "    fig.set_size_inches((DefaultSize[0]*1.2, DefaultSize[1]*1.2))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    #Compute and print label weights\n",
    "    labels = inputs.loc[:,'label']\n",
    "    class_weights = labels.value_counts() / len(labels)\n",
    "    print(class_weights)\n",
    "\n",
    "    #Store description of all features\n",
    "    feature_details = features.describe().transpose()\n",
    "\n",
    "    #Return feature details\n",
    "    return feature_details\n",
    "\n",
    "def find_best_classifier(classifiers, dm_reductions, scorer, X_t, y_t, X_c, y_c, X_v, y_v, cv_sets, params, jobs):\n",
    "    ''' Tune all classifier and dimensionality reduction combiantions to find best classifier. '''\n",
    "\n",
    "    #Initialize result storage\n",
    "    clfs_return = []\n",
    "    dm_reduce_return = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    #Loop through dimensionality reductions\n",
    "    for dm in dm_reductions:\n",
    "\n",
    "        #Loop through classifiers\n",
    "        for clf in clfs:\n",
    "\n",
    "            #Grid search, calibrate, and test the classifier\n",
    "            clf, dm_reduce, train_score, test_score = train_calibrate_predict(clf = clf, dm_reduction = dm, X_train = X_t, y_train = y_t,\n",
    "                                                      X_calibrate = X_c, y_calibrate = y_c,\n",
    "                                                      X_test = X_v, y_test = y_v, cv_sets = cv_sets,\n",
    "                                                      params = params[clf], scorer = scorer, jobs = jobs, use_grid_search = True)\n",
    "\n",
    "            #Append the result to storage\n",
    "            clfs_return.append(clf)\n",
    "            dm_reduce_return.append(dm_reduce)\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "\n",
    "    #Return storage\n",
    "    return clfs_return, dm_reduce_return, train_scores, test_scores\n",
    "\n",
    "def plot_training_results(clfs, dm_reductions, train_scores, test_scores, path):\n",
    "    ''' Plot results of classifier training. '''\n",
    "\n",
    "    #Set graph format\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\", font_scale = 1, rc={\"lines.linewidth\": 1})\n",
    "    ax = plt.subplot(111)\n",
    "    w = 0.5\n",
    "    x = np.arange(len(train_scores))\n",
    "    ax.set_yticks(x + w)\n",
    "    ax.legend((train_scores[0], test_scores[0]), (\"Train Scores\", \"Test Scores\"))\n",
    "    names = []\n",
    "\n",
    "    #Loop throuugh classifiers\n",
    "    for i in range(0, len(clfs)):\n",
    "\n",
    "        #Define temporary variables\n",
    "        clf = clfs[i]\n",
    "        # Access the base estimator through the 'estimator' attribute\n",
    "        clf_name = clf.estimator.__class__.__name__\n",
    "        dm = dm_reductions[i]\n",
    "        dm_name = dm.__class__.__name__\n",
    "\n",
    "        #Create and store name\n",
    "        name = \"{} with {}\".format(clf.estimator.__class__.__name__, dm.__class__.__name__)\n",
    "        names.append(name)\n",
    "\n",
    "    #Plot all names in horizontal bar plot\n",
    "    ax.set_yticklabels((names))\n",
    "    plt.xlim(0.5, 0.55)\n",
    "    plt.barh(x, test_scores, color = 'b', alpha = 0.6)\n",
    "    plt.title(\"Test Data Accuracy Scores\")\n",
    "    fig = plt.figure(1)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def optimize_betting(best_clf, best_dm_reduce, bk_cols_selected, bk_cols, match_data, fifa_data,\n",
    "                     n_samples, sample_size, parameter_1_grid, parameter_2_grid, verbose = False):\n",
    "    ''' Tune parameters of bet selection algorithm. '''\n",
    "\n",
    "    #Generate data samples\n",
    "    samples = []\n",
    "    for i in range(0, n_samples):\n",
    "        sample = match_data.sample(n = sample_size, random_state = 42)\n",
    "        samples.append(sample)\n",
    "\n",
    "    results = pd.DataFrame(columns = [\"parameter_1\", \"parameter_2\", \"results\"])\n",
    "    row = 0\n",
    "\n",
    "    #Iterate over all 1 parameter\n",
    "    for i in parameter_1_grid:\n",
    "\n",
    "        #Iterate over all 2 parameter\n",
    "        for j in parameter_2_grid:\n",
    "\n",
    "            #Compute average score over all samples\n",
    "            profits = []\n",
    "            for sample in samples:\n",
    "                choices = find_good_bets(best_clf, best_dm_reduce, bk_cols_selected, bk_cols, sample, fifa_data, i, j)\n",
    "                profit = execute_bets(choices, match_data)\n",
    "                profits.append(profit)\n",
    "            result = np.mean(np.array(profits))\n",
    "            results.loc[row,\"results\"] = result\n",
    "            results.loc[row,\"parameter_1\"] = i\n",
    "            results.loc[row,\"parameter_2\"] = j\n",
    "            row = row + 1\n",
    "            if verbose == True: print(\"Simulated parameter combination: {}\".format(row))\n",
    "\n",
    "    #Return best setting and result\n",
    "    best_result = results.loc[results['results'].idxmax()]\n",
    "    return best_result\n",
    "\n",
    "\n",
    "def plot_bookkeeper_cf_matrix(matches, bookkeepers, path, verbose=False, normalize=True):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix of bookkeeper predictions.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(\"Obtaining labels...\")\n",
    "\n",
    "    # Get true labels\n",
    "    y_test_temp = matches.apply(get_match_label, axis=1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Obtaining bookkeeper probabilities...\")\n",
    "\n",
    "    # Get bookkeeper probabilities\n",
    "    bookkeeper_probs = get_bookkeeper_probs(matches, bookkeepers)\n",
    "    bookkeeper_probs.reset_index(inplace=True, drop=True)\n",
    "    bookkeeper_probs.dropna(inplace=True)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Obtaining bookkeeper labels...\")\n",
    "\n",
    "    # Get predicted labels from max-probability\n",
    "    y_pred_temp = pd.DataFrame()\n",
    "    y_pred_temp[\"bk_label\"] = bookkeeper_probs[[\"Win\", \"Draw\", \"Defeat\"]].idxmax(axis=1)\n",
    "    y_pred_temp[\"match_api_id\"] = bookkeeper_probs[\"match_api_id\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Plotting confusion matrix...\")\n",
    "\n",
    "    # Join predictions with true labels\n",
    "    results = pd.merge(y_pred_temp, y_test_temp, on=\"match_api_id\", how=\"left\")\n",
    "    y_test = results[\"label\"]\n",
    "    y_pred = results[\"bk_label\"]\n",
    "\n",
    "    # Define label order\n",
    "    labels = [\"Win\", \"Draw\", \"Defeat\"]\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum()\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion matrix of Bookkeeper predictions\")\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print report and accuracy\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Bookkeeper score for test set: {:.4f}.\".format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016d3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = \"data/\"  #Insert path here\n",
    "database = path + 'database.sqlite'\n",
    "conn = sqlite3.connect(database)\n",
    "\n",
    "#Defining the number of jobs to be run in parallel during grid search\n",
    "n_jobs = 1 #Insert number of parallel jobs here\n",
    "\n",
    "#Fetching required data tables\n",
    "player_data = pd.read_sql(\"SELECT * FROM Player;\", conn)\n",
    "player_stats_data = pd.read_sql(\"SELECT * FROM Player_Attributes;\", conn)\n",
    "team_data = pd.read_sql(\"SELECT * FROM Team;\", conn)\n",
    "match_data = pd.read_sql(\"SELECT * FROM Match;\", conn)\n",
    "\n",
    "# Remove special characters\n",
    "def remove_accents(text):\n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    return \"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "team_data[\"team_long_name\"] = team_data[\"team_long_name\"].apply(remove_accents)\n",
    "team_data[\"team_long_name\"] = team_data[\"team_long_name\"].str.replace(r\"[^A-Za-z0-9 ]\", \"\", regex=True)\n",
    "\n",
    "\n",
    "#Reduce match data to fulfill run time requirements\n",
    "rows = [\"country_id\", \"league_id\", \"season\", \"stage\", \"date\", \"match_api_id\", \"home_team_api_id\",\n",
    "        \"away_team_api_id\", \"home_team_goal\", \"away_team_goal\", \"home_player_1\", \"home_player_2\",\n",
    "        \"home_player_3\", \"home_player_4\", \"home_player_5\", \"home_player_6\", \"home_player_7\",\n",
    "        \"home_player_8\", \"home_player_9\", \"home_player_10\", \"home_player_11\", \"away_player_1\",\n",
    "        \"away_player_2\", \"away_player_3\", \"away_player_4\", \"away_player_5\", \"away_player_6\",\n",
    "        \"away_player_7\", \"away_player_8\", \"away_player_9\", \"away_player_10\", \"away_player_11\"]\n",
    "match_data.dropna(subset = rows, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db9116c-8667-4ac5-b7cc-ccfb5fed6789",
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_data = get_fifa_data(match_data, player_stats_data, data_exists = False)\n",
    "\n",
    "#Creating features and labels based on data provided\n",
    "bk_cols = ['B365', 'BW', 'IW', 'LB', 'PS', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "bk_cols_selected = ['B365', 'BW']\n",
    "feables = create_feables(match_data, fifa_data, bk_cols_selected, get_overall = True)\n",
    "inputs = feables.drop('match_api_id', axis = 1)\n",
    "\n",
    "inputs.to_parquet(\"data/inputs.parquet\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c55ed-ef0b-40fc-b47d-bba375519d16",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get Lon/lat per Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1034b-3996-4064-ac3c-5783ca3693b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names_city = pd.read_csv(\"data/team_names_with_cities_v2.csv\")[[\"team_long_name\", \"city\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f96c7-bb39-4510-9b92-6027e0793125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters\n",
    "def remove_accents(text):\n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    return \"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "team_names_city[\"city\"] = team_names_city[\"city\"].apply(remove_accents)\n",
    "team_names_city[\"city\"] = team_names_city[\"city\"].str.replace(r\"[^A-Za-z0-9 ]\", \"\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4926f642-33b5-4a8c-8142-e543ea853ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074928a5-3d0e-477d-8511-4ee96c85edad",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = pd.DataFrame(team_names_city, columns = [\"city\"]).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600f644-0a34-427a-b824-a4499c77efbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07aed460-a8db-4084-9b64-52ac118c0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "from tqdm import tqdm\n",
    "geolocator = Nominatim(user_agent=\"AIzaSyDldImXKRKH3DcI40uDYWf8rAJsHLBRaFs\")\n",
    "def get_lon_lat(city):\n",
    "    try:\n",
    "        location = geolocator.geocode(city)\n",
    "        return location.latitude, location.longitude\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4170543b-357d-4249-83fb-2165b721480b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lat_lst = []\n",
    "lon_lst = []\n",
    "not_found = []\n",
    "cty_lst = []\n",
    "for el in tqdm(cities.city):\n",
    "    try:\n",
    "        lat, lon = get_lon_lat(el)\n",
    "        lat_lst.append(lat)\n",
    "        lon_lst.append(lon)\n",
    "        cty_lst.append(el)\n",
    "    except:\n",
    "        try:\n",
    "            lat, lon = get_lon_lat(el)\n",
    "            lat_lst.append(lat)\n",
    "            lon_lst.append(lon)\n",
    "            cty_lst.append(el)\n",
    "        except:\n",
    "            print(f\"city {el} not found finally\")\n",
    "            not_found.append(el)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f6828e-4c2b-42d0-af83-fb1bee6f248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{len(set(not_found))} Eintr√§ge wurden nicht gefunden\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08300571-998c-49ee-b889-59ed50983f7b",
   "metadata": {},
   "source": [
    "handle not found cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55a460-fd97-4ab7-a9a5-d00b38ee5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59c1fe-75f6-4110-b996-e70df22e11a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = geolocator.geocode(\"Leiria\")\n",
    "print((location.latitude, location.longitude))\n",
    "print(location.address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68be30b-5cf1-4324-aa5e-f46f754b401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually append missing teams\n",
    "teams = {\n",
    "    \"SintTruiden\": {\"city\":\"SintTruiden\",\"lat\":50.81572480,\"lon\": \t5.18625080},\n",
    "    \"StokeonTrent\": {\"city\":\"StokeonTrent\",\"lat\":53.002666,\"lon\":-2.179404},\n",
    "    \"SaintEtienne\": {\"city\":\"SaintEtienne\",\"lat\":45.434700,\"lon\":4.390300},\n",
    "    \"Wrocaw\": {\"city\":\"Wrocaw\",\"lat\":51.107883,\"lon\":17.038538},\n",
    "    \"Wodzisaw\": {\"city\":\"Wodzisaw\",\"lat\":50.00313700,\"lon\":18.47191020},\n",
    "    \"Bechatow\": {\"city\":\"Bechatow\",\"lat\":51.3687535,\"lon\":19.3564248},\n",
    "    \"Biaystok\": {\"city\":\"Biaystok\",\"lat\":53.13248860,\"lon\":23.16884030},\n",
    "    \"eczna\": {\"city\":\"eczna\",\"lat\":52.406376,\"lon\":16.925167},\n",
    "}\n",
    "\n",
    "nf_cities = pd.DataFrame.from_dict(teams, orient=\"index\").reset_index()\n",
    "nf_cities = nf_cities.rename(columns={\"index\": \"team\"})\n",
    "nf_cities = nf_cities[[\"city\", \"lat\", \"lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b8437-db04-4a13-8dd3-44bc83e0d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf_cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c9a814-d41e-40cc-8d8c-cdee4949c09a",
   "metadata": {},
   "source": [
    "handle found cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578e06d-3e3b-4e32-a741-49e53322e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lon_lat = pd.DataFrame(\n",
    "    {\"city\": cty_lst,\n",
    "     \"lat\": lat_lst,\n",
    "     \"lon\": lon_lst}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105a689-4437-4ee3-a4b1-26c088fb19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lon_lat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee59b6f-43a9-45db-b8bf-abcc0d1156fe",
   "metadata": {},
   "source": [
    "bring both together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a9721-b323-4f57-b200-428b9d156940",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lon_lat_final = pd.concat([city_lon_lat, nf_cities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532e02e8-8a1b-4d9b-b73d-7c39b5f27162",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lon_lat_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e470e7f-1658-478b-86cd-57ebacc28fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lon_lat_final.to_parquet(\"data/city_lon_lat_final.parquet\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0ab60b-f375-4200-b47d-298006ddb2ae",
   "metadata": {},
   "source": [
    "# Get games, lat,lon,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71d924-264d-4741-9d99-3ea4e96bbe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lon_lat_final = pd.read_parquet(\"data/city_lon_lat_final.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52307a41-3514-475e-9659-7cc249000326",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_lon_lat_final.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34858f5-e9d3-42d3-bf45-4fc7ee2dbb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names_city.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90987125-77ea-47e8-adec-d0f37c331189",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(team_names_city.city.unique()) - set(city_lon_lat_final.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239bcc05-6406-42b5-b687-34e81e8ccc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(city_lon_lat_final.city.unique()) - set(team_names_city.city.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccfe02-25ab-4d8f-95c7-6c9275598e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names_long_lat = team_names_city.merge(city_lon_lat_final, on=\"city\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdb0c7-b9a9-4e5c-91a5-c66d7ae6be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names_long_lat.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8579bc4-f8e6-4dc5-9520-acd61495d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names_long_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3492c766-ad53-40db-b8c2-92aa58ec233b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd4fe6c4-9e7a-42ed-ac1c-3457cfd400b7",
   "metadata": {},
   "source": [
    "## get game Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4556b-1d44-4c39-b244-7b72e179ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_short = match_data[[\"date\", \"home_team_api_id\", \"away_team_api_id\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5750474-6d09-45b9-9974-ac13797f43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_short_with_team_name = match_data_short.merge(team_data, left_on=\"home_team_api_id\", right_on=\"team_api_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e2840-db81-447f-b4b5-e59ca5142ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_short_with_team_name = match_data_short_with_team_name[[\"date\", \"home_team_api_id\", \"away_team_api_id\", \"team_long_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf6f272-12bd-41d0-961d-5c619707c3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_short_with_team_name.columns = [\"date\", \"home_team_api_id\", \"away_team_api_id\", \"home_team_long_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b291fc5-2eff-4e49-9756-65cb114e7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_short_with_team_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861490d-4d37-43dd-9d9e-41210b0cc662",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat = match_data_short_with_team_name.merge(team_names_long_lat, left_on=\"home_team_long_name\", right_on=\"team_long_name\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c203a9ad-62cf-4d83-9558-5c802fcceff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat = match_data_with_lon_lat[[\"date\", \"home_team_api_id\", \"away_team_api_id\", \"home_team_long_name\", \"city\", \"lat\", \"lon\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06645bc-9764-48d2-9d4d-8efa20094414",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06c367-9c3f-4284-9ed3-8e6830e35849",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d67d6d-f4af-486d-954b-54dc2e31ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat.to_parquet(\"data/match_data_with_lon_lat.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfb1bdd-9a9b-4132-8dd3-7828cdf2967f",
   "metadata": {},
   "source": [
    "# Get weather (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40644af-aa60-49bb-b8d5-8e9af19e0387",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat = pd.read_parquet(\"data/match_data_with_lon_lat.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fe0ecb-90b5-4523-941f-51ee1988a9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import pandas as pd\n",
    "import requests_cache\n",
    "from retry_requests import retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79debc1-872a-4393-b2c5-851f4dc9afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cd077-21d9-46d1-8e44-e87ef2e39047",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(lat,lon,game_date):\n",
    "    url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"start_date\": game_date,\n",
    "        \"end_date\": game_date,\n",
    "        \"daily\": [\"weather_code\", \"rain_sum\"],\n",
    "    }\n",
    "    responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "    # Process first location. Add a for-loop for multiple locations or weather models\n",
    "    response = responses[0]\n",
    "\n",
    "    # Process daily data. The order of variables needs to be the same as requested.\n",
    "    daily = response.Daily()\n",
    "    daily_weather_code = daily.Variables(0).ValuesAsNumpy()\n",
    "    daily_rain_sum = daily.Variables(1).ValuesAsNumpy()\n",
    "\n",
    "    daily_data = {\"date\": pd.date_range(\n",
    "        start = pd.to_datetime(daily.Time(), unit = \"s\", utc = True),\n",
    "        end = pd.to_datetime(daily.TimeEnd(), unit = \"s\", utc = True),\n",
    "        freq = pd.Timedelta(seconds = daily.Interval()),\n",
    "        inclusive = \"left\"\n",
    "    )}\n",
    "\n",
    "    daily_data[\"weather_code\"] = daily_weather_code\n",
    "    daily_data[\"rain_sum\"] = daily_rain_sum\n",
    "\n",
    "    daily_dataframe = pd.DataFrame(data = daily_data)\n",
    "    return daily_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f6fac-ac80-44f8-a14c-71b0c1309423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_for_all_matches(lat, lon, date):\n",
    "    daily_dataframe = get_weather(lat,lon,date)\n",
    "    return daily_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e12ab-f5d8-47c4-8eb6-b07d9f64d2f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6338b52-b299-4700-bb65-34c05249aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54053a5-c9d6-45c2-948b-32c2c520b5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_matches = []\n",
    "for i, row in tqdm(match_data_with_lon_lat.iterrows()):\n",
    "    try:\n",
    "        weather_df = get_weather_for_all_matches(row.lat, row.lon, row.date.split(\" \")[0])\n",
    "        weather_df[\"home_team_api_id\"] = row.home_team_api_id\n",
    "        weather_df[\"away_team_api_id\"] = row.away_team_api_id\n",
    "        weather_df[\"home_team_long_name\"] = row.home_team_long_name\n",
    "        weather_df[\"city\"] = row.city\n",
    "        weather_df[\"lat\"] = row.lat\n",
    "        weather_df[\"lon\"] = row.lon\n",
    "        weather_matches.append(weather_df)\n",
    "        if i%500 == 0:\n",
    "            time.sleep(60)\n",
    "    except:\n",
    "        try:\n",
    "            print(\"Do Sleep\")\n",
    "            time.sleep(3600)\n",
    "            weather_df = get_weather_for_all_matches(row.lat, row.lon, row.date.split(\" \")[0])\n",
    "            weather_df[\"home_team_api_id\"] = row.home_team_api_id\n",
    "            weather_df[\"away_team_api_id\"] = row.away_team_api_id\n",
    "            weather_df[\"home_team_long_name\"] = row.home_team_long_name\n",
    "            weather_df[\"city\"] = row.city\n",
    "            weather_df[\"lat\"] = row.lat\n",
    "            weather_df[\"lon\"] = row.lon\n",
    "            weather_matches.append(weather_df)\n",
    "        except:\n",
    "            print(f\"Fehler bei {i}, {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a0aa12-0234-4def-8798-e1acb70a53c1",
   "metadata": {},
   "source": [
    "Return df: home_team, away_team, date, weather, rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9860b8a2-0063-44ad-aba4-babe9a934346",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_matches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28669929-cdb2-4e28-95ee-27f35002f042",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches_with_weather = pd.concat(weather_matches, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931a267d-21d9-4d54-98ce-d51b84d38c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches_with_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d1c91-1595-4053-bb03-7bc391b88250",
   "metadata": {},
   "source": [
    "### Manuell Fehlende Wetter Daten hinzuf√ºgen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a10bc-021f-424b-b640-7c8f9286c5df",
   "metadata": {},
   "source": [
    "Problem bei Index 20289 --> Nur ein Wert muss manuell geholt werdeN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab82f1f-cbaf-4ba5-8183-09fb62f63a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat.iloc[20289]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129d92e-a6a1-4c75-a8a6-a041fd44f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat.iloc[20289].lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5d1f4-1aa0-4c76-bc4d-b41610b8a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.strptime(match_data_with_lon_lat.iloc[20289].date, \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a563918b-7ce3-4a63-834a-ad9d42f7364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_short_df = get_weather_for_all_matches(match_data_with_lon_lat.iloc[20289].lat, match_data_with_lon_lat.iloc[20289].lon, datetime.strptime(match_data_with_lon_lat.iloc[20289].date, \"%Y-%m-%d %H:%M:%S\").strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb0b876-2f8b-491c-a5ec-ec59f940fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_short_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cb9a34-d740-4407-96ff-d330a0605f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_short_df[\"home_team_api_id\"] = match_data_with_lon_lat.iloc[20289].home_team_api_id\n",
    "weather_short_df[\"away_team_api_id\"] = match_data_with_lon_lat.iloc[20289].away_team_api_id\n",
    "weather_short_df[\"home_team_long_name\"] = match_data_with_lon_lat.iloc[20289].home_team_long_name\n",
    "weather_short_df[\"city\"] = match_data_with_lon_lat.iloc[20289].city\n",
    "weather_short_df[\"lat\"] = match_data_with_lon_lat.iloc[20289].lat\n",
    "weather_short_df[\"lon\"] = match_data_with_lon_lat.iloc[20289].lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421957d3-dd74-42f7-ab3d-3b4756daeb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_short_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab99252f-46b3-4234-bf44-e5c83f0f9cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches_with_weather = pd.read_parquet(\"data/all_matches_with_weather.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73456b1b-795d-45d9-9da8-eccaa94f045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches_with_weather = pd.concat([all_matches_with_weather, weather_short_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f99465-9c1d-4d2d-be01-ca7eff395648",
   "metadata": {},
   "source": [
    "### Save to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6cc33c-32ed-45a9-b317-e4d1441497f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches_with_weather.to_parquet(\"data/all_matches_with_weather.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6534008-c69d-48cf-b442-3f3a29057c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Days since last game for home team (finisehd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a13f7-b86e-4d07-9b84-33275a412545",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d503e4-8833-4955-8d82-d566be37c554",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_sorted = match_data_with_lon_lat.sort_values(\n",
    "    by=[\"home_team_api_id\", \"date\"],\n",
    "    ascending=[True, True]   # beides aufsteigend\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69544be-81e7-4285-b4ae-daa65e2278d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_sorted[\"date\"] = pd.to_datetime(match_data_sorted[\"date\"])\n",
    "\n",
    "match_data_sorted[\"days_since_last_home\"] = (\n",
    "    match_data_sorted.groupby(\"home_team_api_id\")[\"date\"]\n",
    "    .diff()                # Zeitdifferenz innerhalb der Gruppe\n",
    "    .dt.days               # in Tage umrechnen\n",
    "    .fillna(0)             # erstes Spiel -> 0\n",
    "    .astype(int)           # als Integer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0696429-1ab2-4216-a1d1-762591f27383",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d18d3c-710e-44f5-ae76-21f439398a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_sorted.to_parquet(\"data/days_since_last_game_home_team.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abfcf33-a776-45b0-88cd-067eedef9127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfed70fd-7c3d-4a00-b81d-caa3aaa7cf8e",
   "metadata": {},
   "source": [
    "# Mondphase (finished)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69625a65-70c7-4709-96a1-a72f42739e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_with_lon_lat.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2358e015-5a47-4f4c-a08b-67326c6258c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_moon_phase = match_data_with_lon_lat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60c64f1-b075-4652-b4a3-48630f8c73cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astral.moon import phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff53cf0-9213-4f7c-8e93-6159f33b0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moon_phase(date):\n",
    "    return phase(datetime.datetime.strptime(date, \"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8860a5-bdeb-43be-96b7-284db33faa54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3dfd4-fea1-44dc-bd50-b54f6316eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases_num = []\n",
    "for i, row in tqdm(match_data_moon_phase.iterrows()):\n",
    "    phases_num.append(get_moon_phase(row.date.split()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082b4da-682a-46c2-a0e0-64f72d37b984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54900ac-a942-4e01-862e-289e076a5b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_moon_phase[\"moon_phase_num\"] = phases_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b417e-5349-4bb2-bb7e-9c0cdf758fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_moon_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9dc6b-f903-4359-b20d-d62106999d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "    0 ‚Äì 6,99 ‚Äì Neumond\n",
    "    7 ‚Äì 7,99 ‚Äì zunehmender Mond\n",
    "    14 ‚Äì 20,99 ‚Äì Vollmond\n",
    "    21 ‚Äì 27,99 ‚Äì abnehmender Mond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48a038a-02b7-4c12-8359-fe6555b680fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 6.99, 13.99, 20.99, 27.99]   # Grenzen\n",
    "labels = [\"new_moon\", \"waxing_moon\", \"full_moon\", \"wanning_moon\"]  # Namen\n",
    "\n",
    "\n",
    "match_data_moon_phase[\"phase_cat\"] = pd.cut(match_data_moon_phase[\"moon_phase_num\"], bins=bins, labels=labels, include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1904a429-7e4c-4f3b-8d60-a8343fab2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_moon_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb14844-74bc-40f4-ac8d-ebd5daa866f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25684a6-1c05-46a8-99c3-4ec81ae092a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_moon_phase.to_parquet(\"data/match_data_moon_phase.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f412b1-9831-4519-bab9-4a6bd0ce217e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c9a5bd3-3381-430f-ae43-e454ab9ed53a",
   "metadata": {},
   "source": [
    "# Make one df with all external_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187fd2d6-e19b-4756-9b96-dff22c2463b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "moonphases = pd.read_parquet(\"data/match_data_moon_phase.parquet\")\n",
    "weather = pd.read_parquet(\"data/all_matches_with_weather.parquet\")\n",
    "game_pause = pd.read_parquet(\"data/days_since_last_game_home_team.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e98be-6614-4108-a8c4-ba468d3e0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [moonphases, weather, game_pause]:\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae1c651-2f13-45d2-8edd-848be5a3c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moonphases.shape)\n",
    "print(weather.shape)\n",
    "print(game_pause.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e56ffc-277c-4b05-8659-1c08d5a854d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(moonphases.isna().any().any())\n",
    "print(weather.isna().any().any())\n",
    "print(game_pause.isna().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3414f8e6-6d20-4d37-8106-46cc1a946d91",
   "metadata": {},
   "source": [
    "Keine Fehlenden Daten --> Merge m√∂glich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a6c84c-3c44-4210-83bc-9c567e8da166",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(game_pause.date.iloc[0]))\n",
    "print(type(weather.date.iloc[0]))\n",
    "print(type(game_pause.date.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff0d0b-9d8c-4298-9bcb-e20d76e98116",
   "metadata": {},
   "outputs": [],
   "source": [
    "moonphases.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa71e9bf-910b-4cef-830b-f6fdad6dfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a3b67-2294-48bf-a676-cf501578def4",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_pause.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc8564-0df9-49e8-8ce1-90246d2ca82c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb2c8d8-e9e0-4121-9ccf-94977af184c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data = (moonphases.merge(weather, on=[\"date\", \"home_team_api_id\", \"away_team_api_id\"], how=\"inner\").merge(game_pause, on=[\"date\", \"home_team_api_id\", \"away_team_api_id\"], how=\"inner\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b036c2c4-15f1-4e8f-91fa-705b133e1388",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f6cebe-04b1-471d-aac8-d286b72c132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data.isna().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922b7057-0236-44fc-97b7-91e2e1dd04ff",
   "metadata": {},
   "source": [
    "Merge hat geklappt, es sind immernoch alle Zeilen vorhanden und es gibt keine Null Werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00961b44-78ad-40c2-9cf9-4b655e805816",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data_short = external_data[[\"date\", \"home_team_api_id\", \"away_team_api_id\", \"home_team_long_name\", \"city\", \"lat\", \"lon\", \"moon_phase_num\", \"phase_cat\", \"weather_code\", \"rain_sum\", \"days_since_last_home\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af9928-832d-4de9-ae20-8fbdbf21cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_data_short.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa0f66-c7e9-4da0-9d83-eb125f911394",
   "metadata": {},
   "source": [
    "Jetzt merge mit match_data, um die id von match_data zu bekommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ca881-ceab-4c98-946e-6e1ae461baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data_for_merge = match_data[[\"id\", \"date\", \"home_team_api_id\", \"away_team_api_id\"]]\n",
    "match_data_for_merge[\"date\"] = pd.to_datetime(match_data_for_merge[\"date\"], utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e821ce-26aa-453e-bfa2-1a7ca36fc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_external_data_df = external_data_short.merge(match_data_for_merge, on=[\"date\", \"home_team_api_id\", \"away_team_api_id\"], how = \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81081b00-b830-4dad-aac8-b7486a3514ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_external_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0eb440-02a6-4713-9357-c4a841147946",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_external_data_df.to_parquet(\"data/final_external_data_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819cbf66-0d90-42d3-80c7-28dcfdfabcae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MetaStuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
